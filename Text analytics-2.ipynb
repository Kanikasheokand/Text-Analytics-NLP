{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3885c7f4",
   "metadata": {},
   "source": [
    "## OBJECTIVE\n",
    "  \n",
    "The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables that are explained below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2eccf",
   "metadata": {},
   "source": [
    "## Data Extraction :\n",
    "For each of the articles, given in the input.xlsx file, extract the article text and save the extracted article in a text file with URL_ID as its file name.While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb21565",
   "metadata": {},
   "source": [
    "## Data Analysis :\n",
    "For each of the extracted texts from the article, perform textual analysis and compute variables. You need to save the output in CSV format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf4e01",
   "metadata": {},
   "source": [
    "## Variables :\n",
    "<ol>\n",
    "<li>POSITIVE SCORE</li>\n",
    "<li>NEGATIVE SCORE</li>\n",
    "<li>POLARITY SCORE</li>\n",
    "<li>SUBJECTIVITY SCORE</li>\n",
    "<li>AVG SENTENCE LENGTH</li>\n",
    "<li>PERCENTAGE OF COMPLEX WORDS</li>\n",
    "<li>FOG INDEX</li>\n",
    "<li>AVG NUMBER OF WORDS PER SENTENCE</li>\n",
    "<li>COMPLEX WORD COUNT</li>\n",
    "<li>WORD COUNT</li>\n",
    "<li>SYLLABLE PER WORD</li>\n",
    "<li>PERSONAL PRONOUNS</li>\n",
    "<li>AVG WORD LENGTH</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afafcee",
   "metadata": {},
   "source": [
    "### Importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac781e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                  \n",
    "import pandas as pd              \n",
    "from bs4 import BeautifulSoup    \n",
    "from textblob import TextBlob    \n",
    "import textstat                 \n",
    "import openpyxl               \n",
    "import string                  \n",
    "import spacy                \n",
    "import re   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984385e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kanikasheokand/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kanikasheokand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01a875",
   "metadata": {},
   "source": [
    "### Combined stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74daed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined different stopwords text files\n",
    "def read_stopwords_from_file(file_path,encoding='utf-8'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        stopwords = [line.strip() for line in file]\n",
    "    return stopwords\n",
    "\n",
    "def combine_stopwords(files_and_encodings):\n",
    "    combined_stopwords = set()\n",
    "\n",
    "    for file_path, encoding in files_and_encodings:\n",
    "        stopwords = read_stopwords_from_file(file_path, encoding)\n",
    "        combined_stopwords.update(stopwords)\n",
    "\n",
    "    return list(combined_stopwords)\n",
    "\n",
    "# Example: List of files and their corresponding encodings\n",
    "files_and_encodings = [\n",
    "    ('StopWords_Auditor.txt', 'ascii'),\n",
    "    ('StopWords_DatesandNumbers.txt', 'ascii'),\n",
    "    ('StopWords_Generic.txt','ascii'),\n",
    "     ('StopWords_GenericLong.txt','ascii'),\n",
    "      ('StopWords_Geographic.txt','ascii'),\n",
    "       ('StopWords_Names.txt','ascii'),\n",
    "        ('StopWords_Currencies.txt','ISO-8859-1')\n",
    "]\n",
    "\n",
    "\n",
    "# Combine stopwords from different files with different encodings into one list\n",
    "stopwords = combine_stopwords(files_and_encodings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3632c3f",
   "metadata": {},
   "source": [
    "### Positive Words Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0125318",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"positive-words.txt\",\"r\") as pos:\n",
    "    poswords = pos.read().split(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d38a6",
   "metadata": {},
   "source": [
    "### Negative Words Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"negative-words.txt\",\"r\",encoding = \"ISO-8859-1\") as neg:\n",
    "    negwords = neg.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4feee79",
   "metadata": {},
   "source": [
    "### Python program to fetch link and perform required operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d0bb9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "data=[]\n",
    "\n",
    "def text(): \n",
    "\n",
    "    #for fetching data from given input excel sheet #\n",
    "    w_book = openpyxl.load_workbook('Input.xlsx')  \n",
    "    w_sheet = w_book['Sheet1']\n",
    "    \n",
    "    #fetch the url from sheet into url variable # \n",
    "    for i in range (2 , 102):\n",
    "        url = (w_sheet.cell(row=i, column=2).value)\n",
    "    \n",
    "    #We need to pass argument called Headers by passing \"User-Agent\" to the request to bypass the mod-security error.\n",
    "\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"}     \n",
    "        response= requests.get(url,headers=headers )\n",
    "    \n",
    "    # apply BeautifulSoup to fetch only html parser \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "    # fetch title from link\n",
    "        title_element = soup.find('h1', class_=\"entry-title\")\n",
    "        article_title = title_element.get_text() if title_element else \"\"\n",
    "        #alternate code-article_title = soup.find('title').get_text()\n",
    "\n",
    "    \n",
    "    #fetch content from link \n",
    "        article_text = soup.findAll(attrs={'class':'td-post-content'})    \n",
    "        article_text = article_text[0].text.replace('\\n',\" \") if article_text else \"\"\n",
    "        #alternative-code\n",
    "        #article_text = \"\"\n",
    "        #for paragraph in soup.find_all(attrs={'class':'td-post-content'}):\n",
    "            #article_text += paragraph.get_text() + \"\\n\"\n",
    "        \n",
    "    ##Remove punctuation\n",
    "        article_text= article_text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    \n",
    "    #tokenize the data \n",
    "        article_text_tokens = word_tokenize(article_text)\n",
    "        \n",
    "    #remove stopwords\n",
    "        filter_tokens = [word.lower() for word in article_text_tokens if not word.lower() in stopwords]\n",
    "    \n",
    "    #positive dictionary \n",
    "        pos_count = [w for w in filter_tokens if w in poswords]   \n",
    "        Positive_score=len(pos_count)\n",
    "    \n",
    "    #negative dictionary\n",
    "        neg_count = [w for w in filter_tokens if w in negwords]   \n",
    "        Negative_score=len(neg_count)\n",
    "        \n",
    "    #join filter data after removing stpowords  \n",
    "        cleaned_text = ' '.join(filter_tokens)\n",
    "        \n",
    "    #words count \n",
    "        Word_Count=len(cleaned_text)\n",
    "        \n",
    "    #Avg_Sentence_Length  \n",
    "        Avg_Sentence_Length= sum([len(l) for l in re.split(r'[?!.]', cleaned_text) if l.strip()])/len(re.split(r'[?!.]', cleaned_text))\n",
    "    \n",
    "    #calculating fog index using textstat library\n",
    "        Fog_Index=(textstat.gunning_fog(cleaned_text))\n",
    "    \n",
    "    #Avg_Number_of_Words_Per_Sentence\n",
    "        Avg_no_of_words_per_sentence= (len(cleaned_text.split()))/(len(re.split(r'[?!.]', cleaned_text)))\n",
    "        \n",
    "    #function to calculate Syllable Count Per Word excluding word ending with \"ed\" or \"es\"\n",
    "        vowels = \"aeiouy\"\n",
    "        count=0\n",
    "        for word in cleaned_text.split():\n",
    "            word=re.sub(r'(es|ed)$', '', word )\n",
    "            for i in range(len(word)):\n",
    "                if word[i].lower() in vowels:\n",
    "                    count=count+1\n",
    "        syllable_count=count\n",
    "        \n",
    "        def Syllable_per_word(cleaned_text):\n",
    "            if len(cleaned_text.split())==0:\n",
    "                syllable_per_word=0\n",
    "            else:\n",
    "                syllable_per_word= syllable_count/len(cleaned_text.split())\n",
    "            return(syllable_per_word)\n",
    "        Syllable_Per_Word=Syllable_per_word(cleaned_text)\n",
    "    \n",
    "    #function to calculate proper noun in article with help of tagging from nltk lib\n",
    "        def ProperNounExtractor(text):\n",
    "            cou = 0\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence)\n",
    "                tagged = nltk.pos_tag(words)\n",
    "                for (word, tag) in tagged:\n",
    "                    if tag == 'PRP': # If the word is a proper noun\n",
    "                        cou = cou + 1 \n",
    "        \n",
    "            return(cou)\n",
    "        Personal_Pronouns=ProperNounExtractor(article_text)\n",
    "    \n",
    "\n",
    "    #function for sentiment analysis\n",
    "        def sentiment_analysis(text):\n",
    "            sentiment = TextBlob(text).sentiment\n",
    "            return (sentiment.polarity),(sentiment.subjectivity)\n",
    "    \n",
    "        polarity, subjectivity=sentiment_analysis(cleaned_text)\n",
    "    \n",
    "        #complex words count \n",
    "        complex_word=0\n",
    "        for word in cleaned_text.split():\n",
    "            count=0\n",
    "            word=re.sub(r'(es|ed)$', '', word )\n",
    "            for i in range(len(word)):\n",
    "                if word[i].lower() in vowels:\n",
    "                    count+=1\n",
    "                else:\n",
    "                    count+=0\n",
    "            if count > 2:\n",
    "                complex_word+=1\n",
    "        Complex_Words=complex_word        \n",
    "                                 \n",
    "        \n",
    "    # calculate average word length\n",
    "        #total_length=sum(len(word) for word in words)\n",
    "        def Average_Word_count(cleaned_text):\n",
    "            total_words=len(cleaned_text.split())\n",
    "            if total_words==0:\n",
    "                Average_Word_Length=0\n",
    "            else:\n",
    "                Average_Word_Length=len(cleaned_text.replace(' ',''))/total_words\n",
    "            return(Average_Word_Length)\n",
    "        \n",
    "        Average_Word_Length=Average_Word_count(cleaned_text)\n",
    "        \n",
    "    # calculate % of complex word\n",
    "        if Word_Count==0:\n",
    "            Percentage_of_Complex_Word=0\n",
    "        else:\n",
    "            Percentage_of_Complex_Word = Complex_Words / Word_Count * 100\n",
    "    \n",
    "        data.insert(i,[url,Positive_score, Negative_score, polarity,subjectivity, Avg_Sentence_Length,Percentage_of_Complex_Word,Fog_Index, Avg_no_of_words_per_sentence , Complex_Words, Word_Count,Syllable_Per_Word,Average_Word_Length, Personal_Pronouns])\n",
    "        \n",
    "\n",
    "    \n",
    "if __name__ == '__main__' :  \n",
    "    text()\n",
    "        \n",
    "df = pd.DataFrame(data,columns=['url','Positive_score','Negative_score','polarity','subjectivity', 'Avg_Sentence_Length', 'Percentage_of_Complex_Word','Fog_Index', 'Avg_no_of_words_per_sentence' , 'Complex_Words', 'Word_Count','Syllable_Per_Word','Average_Word_Length','Personal_Pronouns'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b55ced",
   "metadata": {},
   "source": [
    "### Dataframe to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2abdae4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>Percentage_of_Complex_Word</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>Avg_no_of_words_per_sentence</th>\n",
       "      <th>Complex_Words</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Syllable_Per_Word</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>0.231311</td>\n",
       "      <td>0.592173</td>\n",
       "      <td>4225.0</td>\n",
       "      <td>5.869822</td>\n",
       "      <td>227.50</td>\n",
       "      <td>559.0</td>\n",
       "      <td>248</td>\n",
       "      <td>4225</td>\n",
       "      <td>2.493739</td>\n",
       "      <td>6.559928</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://insights.blackcoffer.com/estimating-th...</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>0.398032</td>\n",
       "      <td>3973.0</td>\n",
       "      <td>6.191795</td>\n",
       "      <td>205.31</td>\n",
       "      <td>495.0</td>\n",
       "      <td>246</td>\n",
       "      <td>3973</td>\n",
       "      <td>2.646465</td>\n",
       "      <td>7.028283</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>0.087283</td>\n",
       "      <td>0.438645</td>\n",
       "      <td>6695.0</td>\n",
       "      <td>6.377894</td>\n",
       "      <td>326.88</td>\n",
       "      <td>806.0</td>\n",
       "      <td>427</td>\n",
       "      <td>6695</td>\n",
       "      <td>2.782878</td>\n",
       "      <td>7.307692</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.106955</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>6.808732</td>\n",
       "      <td>102.81</td>\n",
       "      <td>238.0</td>\n",
       "      <td>131</td>\n",
       "      <td>1924</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>7.088235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.078596</td>\n",
       "      <td>0.307641</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>6.347746</td>\n",
       "      <td>125.11</td>\n",
       "      <td>303.0</td>\n",
       "      <td>138</td>\n",
       "      <td>2174</td>\n",
       "      <td>2.280528</td>\n",
       "      <td>6.178218</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>31</td>\n",
       "      <td>70</td>\n",
       "      <td>-0.008009</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>6.034783</td>\n",
       "      <td>288.51</td>\n",
       "      <td>712.0</td>\n",
       "      <td>347</td>\n",
       "      <td>5750</td>\n",
       "      <td>2.662921</td>\n",
       "      <td>7.077247</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-inter...</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>0.103238</td>\n",
       "      <td>0.456881</td>\n",
       "      <td>4403.0</td>\n",
       "      <td>7.040654</td>\n",
       "      <td>204.05</td>\n",
       "      <td>483.0</td>\n",
       "      <td>310</td>\n",
       "      <td>4403</td>\n",
       "      <td>3.111801</td>\n",
       "      <td>8.118012</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>79</td>\n",
       "      <td>24</td>\n",
       "      <td>0.044480</td>\n",
       "      <td>0.356677</td>\n",
       "      <td>7534.0</td>\n",
       "      <td>7.074595</td>\n",
       "      <td>346.22</td>\n",
       "      <td>858.0</td>\n",
       "      <td>533</td>\n",
       "      <td>7534</td>\n",
       "      <td>3.122378</td>\n",
       "      <td>7.782051</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://insights.blackcoffer.com/how-neural-ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://insights.blackcoffer.com/covid-19-envi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  Positive_score  \\\n",
       "0   https://insights.blackcoffer.com/rising-it-cit...              33   \n",
       "1   https://insights.blackcoffer.com/estimating-th...              23   \n",
       "2   https://insights.blackcoffer.com/rising-it-cit...              58   \n",
       "3   https://insights.blackcoffer.com/contribution-...               5   \n",
       "4   https://insights.blackcoffer.com/how-covid-19-...              16   \n",
       "..                                                ...             ...   \n",
       "95  https://insights.blackcoffer.com/rise-of-cyber...              31   \n",
       "96  https://insights.blackcoffer.com/rise-of-inter...              29   \n",
       "97  https://insights.blackcoffer.com/rise-of-telem...              79   \n",
       "98  https://insights.blackcoffer.com/how-neural-ne...               0   \n",
       "99  https://insights.blackcoffer.com/covid-19-envi...               0   \n",
       "\n",
       "    Negative_score  polarity  subjectivity  Avg_Sentence_Length  \\\n",
       "0                6  0.231311      0.592173               4225.0   \n",
       "1               27  0.021817      0.398032               3973.0   \n",
       "2               31  0.087283      0.438645               6695.0   \n",
       "3                3  0.106955      0.452500               1924.0   \n",
       "4                3  0.078596      0.307641               2174.0   \n",
       "..             ...       ...           ...                  ...   \n",
       "95              70 -0.008009      0.451500               5750.0   \n",
       "96               9  0.103238      0.456881               4403.0   \n",
       "97              24  0.044480      0.356677               7534.0   \n",
       "98               0  0.000000      0.000000                  0.0   \n",
       "99               0  0.000000      0.000000                  0.0   \n",
       "\n",
       "    Percentage_of_Complex_Word  Fog_Index  Avg_no_of_words_per_sentence  \\\n",
       "0                     5.869822     227.50                         559.0   \n",
       "1                     6.191795     205.31                         495.0   \n",
       "2                     6.377894     326.88                         806.0   \n",
       "3                     6.808732     102.81                         238.0   \n",
       "4                     6.347746     125.11                         303.0   \n",
       "..                         ...        ...                           ...   \n",
       "95                    6.034783     288.51                         712.0   \n",
       "96                    7.040654     204.05                         483.0   \n",
       "97                    7.074595     346.22                         858.0   \n",
       "98                    0.000000       0.00                           0.0   \n",
       "99                    0.000000       0.00                           0.0   \n",
       "\n",
       "    Complex_Words  Word_Count  Syllable_Per_Word  Average_Word_Length  \\\n",
       "0             248        4225           2.493739             6.559928   \n",
       "1             246        3973           2.646465             7.028283   \n",
       "2             427        6695           2.782878             7.307692   \n",
       "3             131        1924           2.571429             7.088235   \n",
       "4             138        2174           2.280528             6.178218   \n",
       "..            ...         ...                ...                  ...   \n",
       "95            347        5750           2.662921             7.077247   \n",
       "96            310        4403           3.111801             8.118012   \n",
       "97            533        7534           3.122378             7.782051   \n",
       "98              0           0           0.000000             0.000000   \n",
       "99              0           0           0.000000             0.000000   \n",
       "\n",
       "    Personal_Pronouns  \n",
       "0                  18  \n",
       "1                  27  \n",
       "2                  22  \n",
       "3                   4  \n",
       "4                  12  \n",
       "..                ...  \n",
       "95                 21  \n",
       "96                  5  \n",
       "97                 27  \n",
       "98                  0  \n",
       "99                  0  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('All_Url.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
